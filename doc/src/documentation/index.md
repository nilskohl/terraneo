# Documentation {#main}

## About

üèóÔ∏è

## Building

### Dependencies

Mandatory:

* MPI (e.g. OpenMPI)

Optional:

* CUDA (for GPU support)

### Compiling on the LMU systems (`cachemiss`, `memoryleak`) for usage with CUDA:

```
$ module load mpi.ompi
$ module load nvidia-hpc

$ mkdir terraneox-build

$ ll
terraneox/               # <== the cloned source code
terraneox-build/

$ cd terraneox-build

$ cmake ../terraneox/ -Kokkos_ENABLE_CUDA=ON

# Build tests
$ cd tests
$ make -j16
```

Note the capitalization: it must be `Kokkos_ENABLE_CUDA=ON`, NOT `KOKKOS_ENABLE_CUDA=ON`.

## Project Structure

```
terraneox/
‚îú‚îÄ‚îÄ apps/                     # Applications (benchmarks, tools, ...) using the framework.
‚îÇ   ‚îú‚îÄ‚îÄ benchmarks/           # Benchmarks ...
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ performance/      # ... run time
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ physics/          # ... physics
‚îÇ   ‚îî‚îÄ‚îÄ tools/                # Tools (e.g., for visualization of meshes)
‚îú‚îÄ‚îÄ data/                     # Stuff that is not exactly library/framework source code
‚îÇ   ‚îî‚îÄ‚îÄ scripts/              # Scripts, e.g., for post-processing
‚îÇ       ‚îî‚îÄ‚îÄ plotting/         # Scripts for visualization, e.g., timing, radial profiles, ... 
‚îú‚îÄ‚îÄ doc/                      # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ documents/            # Documents that are not part of the generated documentation (e.g., Latex documents)
‚îÇ   ‚îî‚îÄ‚îÄ src/                  # Doxygen documentation
‚îÇ       ‚îî‚îÄ‚îÄ documentation/    # Doxygen documentation source (this page is likely here)
‚îú‚îÄ‚îÄ extern/                   # External libraries shipped with this repository
‚îú‚îÄ‚îÄ src/                      # Source code
‚îî‚îÄ‚îÄ tests/                    # Tests
```

## Framework documentation

### Model / Partial differential equations

üèóÔ∏è

-----------

### Grid structure and subdomains (part I - logical structure)

\note This section is just describing the logical organization of data. For details on actual data layout / allocation
      refer to the Kokkos section. Details on the construction of the thick spherical shell and communication are given
      in dedicated sections below.

All grid operations are performed on a set of hexahedral subdomains. Those are organized as 4- or 5 dimensional arrays.

```
    data( local_subdomain_id, node_x, node_y, node_r )                     // for scalar data
    data( local_subdomain_id, node_x, node_y, node_r, vector_entry )       // for vector-valued data 
```

The first index specifies the subdomain. Then the x, y, r coordinates of the nodes (we are using r instead of z because
we will for our application most of the time think in radial directions). For vector-valued data (think velocity vectors
stored at each node) a fifth index specifies the entry of the vector. In the code, the size of that fifth dimension is
typically a compile-time constant.

This layout forces all subdomains to be of equal size.

As described in the finite element section, we mostly use wedge elements. The nodes that span wedge elements have the
same spatial organization of nodes required for hexahedral elements.
The division of one hexahedron into two wedges is implicitly done in the compute kernels and is not represented by the
grid data structure.

(Since we are using Lagrangian basis functions nodes directly correspond to coefficients. For the linear cases, we can
also use the same grid data structure for linear hexahedral finite elements. Such an extension is straightforward and
just requires respective kernels. One could even mix both - although it is not clear if that is mathematically sound.)

As a convention, the hexahedral elements are split into two wedges diagonally from node (1, 0) to node (0, 1) as 
follows:

```
Example of a 5x4 subdomain, that would be extruded in r-direction.
Each node 'o' either stores a scalar or a vector. 

        o---o---o---o---o
        |\  |\  |\  |\  |
        | \ | \ | \ | \ |
        o---o---o---o---o
        |\  |\  |\  |\  |
        | \ | \ | \ | \ |
        o---o---o---o---o
   ^    |\  |\  |\  |\  |
   |    | \ | \ | \ | \ |
  y|    o---o---o---o---o
   
        -->
        x
```

Some helper functions to work on this grid structure are supplied in `kernel_helpers.hpp`. 

-----------

### Finite element discretization

The partial differential equations and their solutions are approximated using the finite element methods.

We are using linear wedge elements for all spaces, unless specified otherwise. This is very similar to the 
implementation in Terra.

\note
See helper functions and documentation in [integrands.hpp](@ref integrands.hpp) or the 
[namespace terra::fe:wedge](@ref terra::fe::wedge) for details, and other, derived
quantities like gradients, Jacobians, determinants, etc.

Linear wedge (or prismatic) elements are formed by extruding a linear triangular element in the radial direction.
The base triangle lies in the lateral plane (parameterized by \f$\xi\f$,\f$\eta\f$), while the extrusion occurs along the radial
coordinate \f$\zeta\f$.

\note
The provided functions for the computation of the gradients, Jacobians, etc. assume that we are working on a spherical
shell and accordingly work with respect to a forward map that maps the reference element onto a wedge with the two
triangular surfaces living on two shell-slices, and the connecting beams being radially extruded from the origin.

#### Geometry

Lateral reference coordinates:
  \f[ \xi, \eta \in [0, 1] \f]

Radial reference coordinates:
  \f[ \zeta \in [-1, 1] \f]

With
  \f[ 0 \leq \xi + \eta \leq 1 \f]

#### Node enumeration

  \code

  r_node_idx = r_cell_idx + 1 (outer):

  5
  |\
  | \
  3--4
  \endcode

  \code

  r_node_idx = r_cell_idx (inner):

  2
  |\
  | \
  0--1
  \endcode

#### Shape functions

Lateral:

  \f[
  \begin{align}
    N^\mathrm{lat}_0 = N^\mathrm{lat}_3 &= 1 - \xi - \eta \\
    N^\mathrm{lat}_1 = N^\mathrm{lat}_4 &= \xi \\
    N^\mathrm{lat}_2 = N^\mathrm{lat}_5 &= \eta
  \end{align}
  \f]

Radial:

  \f[
  \begin{align}
    N^\mathrm{rad}_0 = N^\mathrm{rad}_1 = N^\mathrm{rad}_2 &= \frac{1}{2} ( 1 - \zeta ) \\
    N^\mathrm{rad}_3 = N^\mathrm{rad}_4 = N^\mathrm{rad}_5 &= \frac{1}{2} ( 1 + \zeta ) \\
  \end{align}
  \f]

Full:

  \f[
  N_i = N^\mathrm{lat}_i N^\mathrm{rad}_i
  \f]


#### Physical coordinates

  \code
  r_1, r_2                     radii of bottom and top (r_1 < r_2)
  p1_phy, p2_phy, p3_phy       coords of triangle on unit sphere
  \endcode


#### Spaces:

For the Stokes system we employ the stable (\f$P_1\f$-iso-\f$P_2\f$, \f$P_1\f$) finite element pairing, i.e., both
the velocity and pressure are discretized with linear wedge elements, with the velocity living on a grid with additional
refinement compared to the pressure grid.

-----------

### Linear algebra

üèóÔ∏è

#### Coefficient vectors

üèóÔ∏è

#### Operators

üèóÔ∏è

#### Solvers

üèóÔ∏è

#### Multigrid

üèóÔ∏è

-----------

### Grid structure and subdomains (part II - Kokkos)

üèóÔ∏è

-----------

### Thick spherical shell

The Earth mantle is approximated via a thick spherical shell \f$\Omega\f$ , i.e., a hollow sphere centered at the origin

\f[ \Omega = \{\mathbf{x} \in \mathbb{R}^3 : r_\mathrm{min} \leq \|\mathbf{x}\| \leq r_\mathrm{max} \} \f]

#### Mesh structure

A corresponding mesh is constructed by splitting the outer surface of \f$\Omega\f$ into 10 spherical diamonds that are 
extruded towards (or equivalently away from) the origin.

The figures below show the diamonds in a three-dimensional visualization (each diamond is refined 4 times in lateral and
4 times in radial direction).

Single diamond (diamond id = 0):
\htmlonly
<video width="960" controls>
<source src="diamond_animation.mp4" type="video/mp4">
</video>
\endhtmlonly
\image html figures/diamond_animation.mp4

Northern (diamonds 0 to 4) and southern diamonds (diamonds 5 to 9):
\htmlonly
<video width="960" controls>
<source src="north_south_animation.mp4" type="video/mp4">
</video>
\endhtmlonly
\image html figures/north_south_animation.mp4

Unfolding the surface partitioning, we can visualize the surface of the 10 spherical diamonds as a net that when curved 
and pieced together recovers the spherical shell: 

\image html figures/thick-spherical-shell-diamond-net.jpg

Note that the extrusion in radial direction is not visible from the net.

Each diamond can (optionally) be subdivided in lateral and radial direction. The radial refinement is straightforward.
After (uniform) lateral refinement, each subdomain can be associated with a globally unique identifying tuple

``` 
subdomain_id = (diamond_id, subdomain_x, subdomain_y, subdomain_r)
```

as illustrated in the figure below:

\image html figures/thick-spherical-shell-subdomains.jpg

The `subdomain_id` is implemented in the class \ref terra::grid::shell::SubdomainInfo.

A domain can be set up using the \ref terra::grid::shell::DomainInfo class.
This does not compute any node coordinates. It just stores the refinement information, i.e., how many subdomains
are present for each diamond in either direction.
In lateral direction, refinement currently has to be uniform.
In radial direction, the concrete radii of the layers can be specified.
For more details refer to the documentation of \ref terra::grid::shell::DomainInfo.

#### Local subdomains

Subdomains on the same MPI process are sorted by their global `subdomain_id` (it is sortable and globally unique)
and continuously assigned to an integer `local_subdomain_id` that ranges from 0 to the number of process-local 
subdomains minus 1.
The `local_subdomain_id` is then the first index of the 4D (or 5D) data grids introduced above.
For instance for a scalar data array `data` the expression
```
    data( 3, 55, 20, 4 )
```
accesses the node with
```
    local_subdomain_id =  3
    x_index            = 55
    y_index            = 20
    r_index            =  4
```
The mapping from the `subdomain_id` (`SubdomainInfo`) to the `local_subdomain_id` (`int`) is performed during set up
and stored together with other information in the corresponding \ref terra::grid::shell::DistributedDomain instance.
More details can be found the parallelization section.

#### Node coordinates

The concrete coordinates of the nodes are computed with two functions:
* \ref terra::grid::shell::subdomain_unit_sphere_single_shell_coords - computes the "lateral cartesian coordinates"
  of all nodes, i.e., computes the cartesian coordinates of a single shell of nodes with radius 1 and returns them in
  a 4D array `coords_shell( local_subdomain_id, x_index, y_index, cartesian_coord )`.
* \ref terra::grid::shell::subdomain_shell_radii - computes the radii and stores them in a 2D array
  `coords_radii( local_subdomain_id, r_index )`
The cartesian coordinate of a node `( local_subdomain_id, x_index, y_index, r_index )` can then be computed via
```
    Vec3 cartesian_coords;
    cartesian_coords( 0 ) = coords_shell( local_subdomain_id, x_index, y_index, 0 );
    cartesian_coords( 1 ) = coords_shell( local_subdomain_id, x_index, y_index, 1 );
    cartesian_coords( 2 ) = coords_shell( local_subdomain_id, x_index, y_index, 2 );
    return cartesian_coords * coords_radii( local_subdomain_id, r_index );
```
This is implemented in \ref terra::grid::shell::coords.
The radius is obviously just `coords_radii( local_subdomain_id, r_index )`.

-----------

### Parallelization

üèóÔ∏è

### Communication

For operations that do not only work locally (such as matrix-vector products) information has to be communicated
across boundaries of neighboring subdomains.
At subdomain boundaries, mesh nodes are duplicated: the same mesh node exists on multiple subdomains. 

Generally, we **assume that the values at the mesh nodes are holding the correct values whenever entering linear 
algebra building blocks**. That means we have to ensure that data is communicated after computations such as 
matrix-vector multiplications.

A map of neighboring subdomains and metadata is generated via the class \ref terra::grid::shell::SubdomainNeighborhood.
That is done internally in the \ref terra::grid::shell::DistributedDomain.  

#### Vector-vector operations (excluding dot products)

Vector-vector operations (such as daxpy etc.) do not require any communication as long as the duplicated nodes are 
updated for each subdomain. Technically that means we perform redundant computations at the benefit of avoiding 
communication and having very simple kernels (you can just loop over the entire subdomain without conditionals).

#### Dot products (and other reductions)

The computation of dot products and other reductions must be performed carefully since we must not include duplicated 
nodes twice. To ensure that, we store a flag field (mostly called mask data in the code) that assigns in a setup phase
(typically once at the start of the program) an `owned` flag to exactly one of the duplicated nodes. The dot
product kernel (or any kind of reduction) then skips the nodes that are not marked as `owned`. See also the section
on flag fields / masks.

#### Assigning random values

One has to take care when assigning random values to all nodes in parallel since after such a randomization, duplicated
nodes must not have different values.

#### Matrix-vector operations

Due to the linearity of the matrix-free finite element matrix-vector multiplication kernels, we can simply (again,
assuming the source vector is already updated) apply the kernel locally and then sum up the values on duplicated nodes
and write that sum to all duplicated nodes.

After the kernel has been executed, that additive communication is performed using two buffers (send and recv) for
each interface that a local subdomain has with another subdomain.
The boundary data is written to the buffer and sent to the receiver side (via MPI).
After receiving the data from the other subdomains, that data is added from the buffers to the local subdomain for each
neighboring subdomain. 
Subdomain faces only have at most one interface with a different subdomain, whereas there can be more than one neighbor
for edges and vertices of a subdomain.

In some cases, the data has to be rotated in some way to match the nodes at the receiver side.
The convention here is that data is packed without rotation and is properly rotated during unpacking.

By coincidence, the subdomain structure of the thick spherical shell only ever requires a small subset of 
rotations. Note that vertex-vertex interfaces are straightforward (no rotation required since only data of a single 
node is sent). Edge-edge interfaces only require checking for one rotation type (either we unpack forward, or backward).
That is also quite simple.
Face-face interfaces technically have more degrees of freedom. However let's look at all cases:

##### Radial direction

This is the simplest case because the iteration pattern is the same in x and y direction. So the pattern is
```
   send_data( local_subdomain_id_sender, x, y, FIXED_TOP_OR_BOTTOM )
=> buffer( x, y )
=> recv_data( local_subdomain_id_recver, x, y, FIXED_BOTTOM_OR_TOP )
```

##### Lateral direction (same diamond)

If both subdomains are in the same diamond, lateral communication is also straightforward.
We keep the radial dimension the second one in the buffer.

```
Either 

   send_data( local_subdomain_id_sender, x, FIXED_START_OR_END, r )
=> buffer( x, r )
=> recv_data( local_subdomain_id_recver, x, FIXED_END_OR_START, r )

or 

   send_data( local_subdomain_id_sender, FIXED_START_OR_END, y, r )
=> buffer( y, r )
=> recv_data( local_subdomain_id_recver, FIXED_END_OR_START, y, r )
```

##### Lateral direction (at diamond-diamond interfaces)

It turns out that we only need a handful of simple rotations due to the structure of the diamonds.
This is nice and makes the communication really easy to implement for our special case.

```

NORTH-NORTH and SOUTH-SOUTH
===========================

Communication between diamonds at the same poles.

=> No rotation necessary. Just "sort into the other coordinate". 

d_0( 0, :, r ) = d_1( :, 0, r )
d_1( 0, :, r ) = d_2( :, 0, r )
d_2( 0, :, r ) = d_3( :, 0, r )
d_3( 0, :, r ) = d_4( :, 0, r )
d_4( 0, :, r ) = d_0( :, 0, r )

d_5( 0, :, r ) = d_6( :, 0, r )
d_6( 0, :, r ) = d_7( :, 0, r )
d_7( 0, :, r ) = d_8( :, 0, r )
d_8( 0, :, r ) = d_9( :, 0, r )
d_9( 0, :, r ) = d_5( :, 0, r )

E.g.:

   send_data_d_1( local_subdomain_id_sender, 0, y, r )
=> buffer( y, r )
=> recv_data_d_2( local_subdomain_id_recver, y, 0, r )

--------------------------------------------------------------------------------

NORTH-SOUTH and SOUTH-NORTH
===========================

Communication between diamonds at different poles.

=> Rotate the x/y direction during unpacking.

d_0( :, end, r ) = d_5( end, :, r )
d_1( :, end, r ) = d_6( end, :, r )
d_2( :, end, r ) = d_7( end, :, r )
d_3( :, end, r ) = d_8( end, :, r )
d_4( :, end, r ) = d_9( end, :, r )

d_5( :, end, r ) = d_1( end, :, r )
d_6( :, end, r ) = d_2( end, :, r )
d_7( :, end, r ) = d_3( end, :, r )
d_8( :, end, r ) = d_4( end, :, r )
d_9( :, end, r ) = d_0( end, :, r )

E.g.:

   send_data_d_1( local_subdomain_id_sender, x, y_size - 1, r )
=> buffer( x, r )
=> recv_data_d_6( local_subdomain_id_recver, x_size - 1, y_size - 1 - x, r )
                                                         ---------------
                                                         ^^^^^^^^^^^^^^^
                                                          rotating here
```

-----------

### Flag fields and masks

üèóÔ∏è

-----------

### Boundary conditions

#### Strong Dirichlet boundary condition enforcement

\note
See
also: [strong_algebraic_dirichlet_enforcement_poisson_like()](@ref terra::fe::strong_algebraic_dirichlet_enforcement_poisson_like)
and similar functions in that file.

<p></p>

\note
A great explanation is given in [Wolfgang Bangerth's
lectures](https://www.math.colostate.edu/~bangerth/videos.676.21.65.html).

Consider the linear problem

\f[ Lu = f \f]

with Dirichlet boundary conditions.

We approach the elimination as follows (assuming interpolating FE spaces).

Let \f$ A \f$ be the "Neumann" operator matrix of \f$ L \f$, i.e., we do not treat the boundaries any differently and
just execute the volume integrals.

1. Interpolate Dirichlet boundary conditions into a vector \f$ g \f$.
2. Compute \f$ g_A \gets A g \f$.
3. Compute \f$ g_D \gets \mathrm{diag}(A) g \f$
4. Set the rhs to \f$ b_\text{elim} = b - g_A \f$,
   where \f$ b \f$ is the assembled rhs vector for the homogeneous problem
   (the result of evaluating the linear form into a vector or of the matrix-vector product of a vector
   \f$ f_\text{vec} \f$ where
   the rhs function \f$ f \f$ has been interpolated into, and then \f$ b = M f_\text{vec} \f$ (\f$ M \f$ being the mass
   matrix))
5. Set the rhs \f$ b_elim \f$ at the boundary nodes to \f$ g_D \f$, i.e.
   \f$ b_\text{elim} \gets g_D \f$ on the Dirichlet boundary
6. Solve
   \f$ A_\text{elim} x = b_\text{elim} \f$
   where \f$ A_\text{elim} \f$ is \f$ A \f$, but with all off-diagonal entries in the same row/col as a boundary node
   set to zero.
   This feature has to be supplied by the operator implementation.
   In a matrix-free context, we have to adapt the element matrix \f$ A_\text{local} \f$ accordingly by (symmetrically)
   zeroing
   out all the off-diagonals (row and col) that correspond to a boundary node. But we keep the diagonal intact.
   We still have \f$ \mathrm{diag}(A) = \mathrm{diag}(A_\text{elim}) \f$ .
7. \f$ x \f$ is the solution of the original problem. No boundary correction should be necessary.

All of this is covered
by [strong_algebraic_dirichlet_enforcement_poisson_like()](@ref terra::fe::strong_algebraic_dirichlet_enforcement_poisson_like)
and similar functions. What needs to be supplied are the operators \f$ A \f$, \f$ \mathrm{diag}(A) \f$, and
\f$ A_\text{elim} \f$. The main "difficulty" is to implement \f$ A_\text{elim} \f$ in a matrix-free kernel.
This can be achieved if one realizes that zeroing out the offdiagonals of the global matrix is equivalently done
by zeroing out the respective entries of the local element matrices.

#### Free-slip

üèóÔ∏è

#### Plate boundaries

üèóÔ∏è

-----------

### IO

#### Tabular data

üèóÔ∏è

#### XDMF

\note Putting the string 'VTK' here in case you are looking for it via full-text search. We are using XDMF instead.

üèóÔ∏è

#### Radial profiles

üèóÔ∏è
